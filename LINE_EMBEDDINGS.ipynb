{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d02aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c dglteam dgl -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3239e61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "\n",
    "dataset = PygNodePropPredDataset(name = \"ogbn-products\", root = 'dataset/')\n",
    " \n",
    "split_idx = dataset.get_idx_split()\n",
    "data = dataset[0]\n",
    "\n",
    "# Convert split indices to boolean masks and add them to `data`.\n",
    "for key, idx in split_idx.items():\n",
    "    mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    mask[idx] = True\n",
    "    data[f'{key}_mask'] = mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62bd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.data.utils import download, _get_dgl_url, get_download_dir, extract_archive\n",
    "import random\n",
    "import time\n",
    "import dgl\n",
    "\n",
    "from torch_geometric.utils import to_undirected, dropout_adj, degree\n",
    "\n",
    "\n",
    "def make_undirected(G):\n",
    "    #G.readonly(False)\n",
    "    G.add_edges(G.edges()[1], G.edges()[0])\n",
    "    return G\n",
    "\n",
    "def find_connected_nodes(G):\n",
    "    nodes = torch.nonzero(G.out_degrees(), as_tuple=False).squeeze(-1)\n",
    "    return nodes\n",
    "\n",
    "class EdgeSampler(object):\n",
    "    def __init__(self, G, seeds):\n",
    "        self.G = G\n",
    "        self.seeds = seeds\n",
    "        self.edges = torch.cat((self.G.edges()[0].unsqueeze(0), self.G.edges()[1].unsqueeze(0)), 0).t()\n",
    "    \n",
    "    def sample(self, seeds):\n",
    "        \"\"\" seeds torch.LongTensor : a batch of indices of edges \"\"\"\n",
    "        return self.edges[torch.LongTensor(seeds)]\n",
    "\n",
    "class LineDataset:\n",
    "    def __init__(self, \n",
    "            data,\n",
    "            batch_size,\n",
    "            num_samples,\n",
    "            negative=5,\n",
    "            gpus=[0],\n",
    "            fast_neg=True,\n",
    "            ):\n",
    "        self.batch_size = batch_size\n",
    "        self.negative = negative\n",
    "        self.num_samples = num_samples\n",
    "        self.num_procs = len(gpus)\n",
    "        self.fast_neg = fast_neg\n",
    "        \n",
    "        self.G = data\n",
    "        self.G = make_undirected(self.G)\n",
    "        print(\"Finish reading graph\")\n",
    "\n",
    "        self.num_nodes = self.G.number_of_nodes()\n",
    "\n",
    "        seeds = np.random.choice(np.arange(self.G.number_of_edges()), \n",
    "                            self.num_samples, \n",
    "                            replace=True) # edge index\n",
    "        self.seeds = torch.split(torch.LongTensor(seeds), \n",
    "            int(np.ceil(self.num_samples / self.num_procs)), \n",
    "            0)\n",
    "        print(\"generate %d samples\" % (len(seeds)))\n",
    "\n",
    "        # negative table for true negative sampling\n",
    "        self.valid_nodes = find_connected_nodes(self.G)\n",
    "        if not fast_neg:\n",
    "            node_degree = self.G.out_degrees(self.valid_nodes).numpy()\n",
    "            node_degree = np.power(node_degree, 0.75)\n",
    "            node_degree /= np.sum(node_degree)\n",
    "            node_degree = np.array(node_degree * 1e8, dtype=np.int)\n",
    "            self.neg_table = []\n",
    "            \n",
    "            for idx, node in enumerate(self.valid_nodes):\n",
    "                self.neg_table += [node] * node_degree[idx]\n",
    "            self.neg_table_size = len(self.neg_table)\n",
    "            self.neg_table = np.array(self.neg_table, dtype=np.long)\n",
    "            del node_degree\n",
    "\n",
    "    def create_sampler(self, i):\n",
    "        \"\"\" create random walk sampler \"\"\"\n",
    "        return EdgeSampler(self.G, self.seeds[i])\n",
    "\n",
    "    def save_mapping(self, map_file):\n",
    "        with open(map_file, \"wb\") as f:\n",
    "            pickle.dump(self.node2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4c9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import random\n",
    "import numpy as np\n",
    "import dgl.multiprocessing as mp\n",
    "from dgl.multiprocessing import Queue\n",
    "\n",
    "def init_emb2neg_index(negative, batch_size):\n",
    "    '''select embedding of negative nodes from a batch of node embeddings \n",
    "    for fast negative sampling\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    index_emb_negu torch.LongTensor : the indices of u_embeddings\n",
    "    index_emb_negv torch.LongTensor : the indices of v_embeddings\n",
    "    Usage\n",
    "    -----\n",
    "    # emb_u.shape: [batch_size, dim]\n",
    "    batch_emb2negu = torch.index_select(emb_u, 0, index_emb_negu)\n",
    "    '''\n",
    "    idx_list_u = list(range(batch_size)) * negative\n",
    "    idx_list_v = list(range(batch_size)) * negative\n",
    "    random.shuffle(idx_list_v)\n",
    "\n",
    "    index_emb_negu = torch.LongTensor(idx_list_u)\n",
    "    index_emb_negv = torch.LongTensor(idx_list_v)\n",
    "\n",
    "    return index_emb_negu, index_emb_negv\n",
    "\n",
    "def adam(grad, state_sum, nodes, lr, device, only_gpu):\n",
    "    \"\"\" calculate gradients according to adam \"\"\"\n",
    "    grad_sum = (grad * grad).mean(1)\n",
    "    if not only_gpu:\n",
    "        grad_sum = grad_sum.cpu()\n",
    "    state_sum.index_add_(0, nodes, grad_sum) # cpu\n",
    "    std = state_sum[nodes].to(device)  # gpu\n",
    "    std_values = std.sqrt_().add_(1e-10).unsqueeze(1)\n",
    "    grad = (lr * grad / std_values) # gpu\n",
    "\n",
    "    return grad\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    \"\"\" Negative sampling based skip-gram \"\"\"\n",
    "    def __init__(self, \n",
    "        emb_size, \n",
    "        emb_dimension,\n",
    "        batch_size,\n",
    "        only_cpu,\n",
    "        only_gpu,\n",
    "        only_fst,\n",
    "        only_snd,\n",
    "        neg_weight,\n",
    "        negative,\n",
    "        lr,\n",
    "        lap_norm,\n",
    "        fast_neg,\n",
    "        num_threads,\n",
    "        mix=False,\n",
    "        record_loss=False,\n",
    "        async_update=False,\n",
    "        ):\n",
    "        \"\"\" initialize embedding on CPU \n",
    "        Paremeters\n",
    "        ----------\n",
    "        emb_size int : number of nodes\n",
    "        emb_dimension int : embedding dimension\n",
    "        batch_size int : number of node sequences in each batch\n",
    "        only_cpu bool : training with CPU\n",
    "        only_gpu bool : training with GPU\n",
    "        only_fst bool : only embedding for first-order proximity\n",
    "        only_snd bool : only embedding for second-order proximity\n",
    "        mix bool : mixed training with CPU and GPU\n",
    "        negative int : negative samples for each positve node pair\n",
    "        neg_weight float : negative weight\n",
    "        lr float : initial learning rate\n",
    "        lap_norm float : weight of laplacian normalization\n",
    "        fast_neg bool : do negative sampling inside a batch\n",
    "        record_loss bool : print the loss during training\n",
    "        use_context_weight : give different weights to the nodes in a context window\n",
    "        async_update : asynchronous training\n",
    "        \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.batch_size = batch_size\n",
    "        self.only_cpu = only_cpu\n",
    "        self.only_gpu = only_gpu\n",
    "        if only_fst:\n",
    "            self.fst = True\n",
    "            self.snd = False\n",
    "            self.emb_dimension = emb_dimension\n",
    "        elif only_snd:\n",
    "            self.fst = False\n",
    "            self.snd = True\n",
    "            self.emb_dimension = emb_dimension\n",
    "        else:\n",
    "            self.fst = True\n",
    "            self.snd = True\n",
    "            self.emb_dimension = int(emb_dimension / 2)\n",
    "        self.mixed_train = mix\n",
    "        self.neg_weight = neg_weight\n",
    "        self.negative = negative\n",
    "        self.lr = lr\n",
    "        self.lap_norm = lap_norm\n",
    "        self.fast_neg = fast_neg\n",
    "        self.record_loss = record_loss\n",
    "        self.async_update = async_update\n",
    "        self.num_threads = num_threads\n",
    "        \n",
    "        # initialize the device as cpu\n",
    "        self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # embedding\n",
    "        initrange = 1.0 / self.emb_dimension\n",
    "        if self.fst:\n",
    "            self.fst_u_embeddings = nn.Embedding(\n",
    "                self.emb_size, self.emb_dimension, sparse=True)\n",
    "            init.uniform_(self.fst_u_embeddings.weight.data, -initrange, initrange)\n",
    "        if self.snd:\n",
    "            self.snd_u_embeddings = nn.Embedding(\n",
    "                self.emb_size, self.emb_dimension, sparse=True)\n",
    "            init.uniform_(self.snd_u_embeddings.weight.data, -initrange, initrange)\n",
    "            self.snd_v_embeddings = nn.Embedding(\n",
    "                self.emb_size, self.emb_dimension, sparse=True)\n",
    "            init.constant_(self.snd_v_embeddings.weight.data, 0)\n",
    "\n",
    "        # lookup_table is used for fast sigmoid computing\n",
    "        self.lookup_table = torch.sigmoid(torch.arange(-6.01, 6.01, 0.01))\n",
    "        self.lookup_table[0] = 0.\n",
    "        self.lookup_table[-1] = 1.\n",
    "        if self.record_loss:\n",
    "            self.logsigmoid_table = torch.log(torch.sigmoid(torch.arange(-6.01, 6.01, 0.01)))\n",
    "            self.loss_fst = []\n",
    "            self.loss_snd = []\n",
    "\n",
    "        # indexes to select positive/negative node pairs from batch_walks\n",
    "        self.index_emb_negu, self.index_emb_negv = init_emb2neg_index(self.negative, self.batch_size)\n",
    "\n",
    "        # adam\n",
    "        if self.fst:\n",
    "            self.fst_state_sum_u = torch.zeros(self.emb_size)\n",
    "        if self.snd:\n",
    "            self.snd_state_sum_u = torch.zeros(self.emb_size)\n",
    "            self.snd_state_sum_v = torch.zeros(self.emb_size)\n",
    "\n",
    "\n",
    "    def share_memory(self):\n",
    "        \"\"\" share the parameters across subprocesses \"\"\"\n",
    "        if self.fst:\n",
    "            self.fst_u_embeddings.weight.share_memory_()\n",
    "            self.fst_state_sum_u.share_memory_()\n",
    "        if self.snd:\n",
    "            self.snd_u_embeddings.weight.share_memory_()\n",
    "            self.snd_v_embeddings.weight.share_memory_()\n",
    "            self.snd_state_sum_u.share_memory_()\n",
    "            self.snd_state_sum_v.share_memory_()\n",
    "\n",
    "    def set_device(self, gpu_id):\n",
    "        \"\"\" set gpu device \"\"\"\n",
    "        self.device = torch.device(\"cuda:%d\" % gpu_id)\n",
    "        print(\"The device is\", self.device)\n",
    "        self.lookup_table = self.lookup_table.to(self.device)\n",
    "        if self.record_loss:\n",
    "            self.logsigmoid_table = self.logsigmoid_table.to(self.device)\n",
    "        self.index_emb_negu = self.index_emb_negu.to(self.device)\n",
    "        self.index_emb_negv = self.index_emb_negv.to(self.device)\n",
    "\n",
    "    def all_to_device(self, gpu_id):\n",
    "        \"\"\" move all of the parameters to a single GPU \"\"\"\n",
    "        self.device = torch.device(\"cuda:%d\" % gpu_id)\n",
    "        self.set_device(gpu_id)\n",
    "        if self.fst:\n",
    "            self.fst_u_embeddings = self.fst_u_embeddings.cuda(gpu_id)\n",
    "            self.fst_state_sum_u = self.fst_state_sum_u.to(self.device)\n",
    "        if self.snd:\n",
    "            self.snd_u_embeddings = self.snd_u_embeddings.cuda(gpu_id)\n",
    "            self.snd_v_embeddings = self.snd_v_embeddings.cuda(gpu_id)\n",
    "            self.snd_state_sum_u = self.snd_state_sum_u.to(self.device)\n",
    "            self.snd_state_sum_v = self.snd_state_sum_v.to(self.device)\n",
    "\n",
    "    def fast_sigmoid(self, score):\n",
    "        \"\"\" do fast sigmoid by looking up in a pre-defined table \"\"\"\n",
    "        idx = torch.floor((score + 6.01) / 0.01).long()\n",
    "        return self.lookup_table[idx]\n",
    "\n",
    "    def fast_logsigmoid(self, score):\n",
    "        \"\"\" do fast logsigmoid by looking up in a pre-defined table \"\"\"\n",
    "        idx = torch.floor((score + 6.01) / 0.01).long()\n",
    "        return self.logsigmoid_table[idx]\n",
    "\n",
    "    def fast_pos_bp(self, emb_pos_u, emb_pos_v, first_flag):\n",
    "        \"\"\" get grad for positve samples \"\"\"\n",
    "        pos_score = torch.sum(torch.mul(emb_pos_u, emb_pos_v), dim=1)\n",
    "        pos_score = torch.clamp(pos_score, max=6, min=-6)\n",
    "        # [batch_size, 1]\n",
    "        score = (1 - self.fast_sigmoid(pos_score)).unsqueeze(1)\n",
    "        if self.record_loss:\n",
    "            if first_flag:\n",
    "                self.loss_fst.append(torch.mean(self.fast_logsigmoid(pos_score)).item())\n",
    "            else:\n",
    "                self.loss_snd.append(torch.mean(self.fast_logsigmoid(pos_score)).item())\n",
    "\n",
    "        # [batch_size, dim]\n",
    "        if self.lap_norm > 0:\n",
    "            grad_u_pos = score * emb_pos_v + self.lap_norm * (emb_pos_v - emb_pos_u)               \n",
    "            grad_v_pos = score * emb_pos_u + self.lap_norm * (emb_pos_u - emb_pos_v)\n",
    "        else:\n",
    "            grad_u_pos = score * emb_pos_v\n",
    "            grad_v_pos = score * emb_pos_u\n",
    "\n",
    "        return grad_u_pos, grad_v_pos\n",
    "\n",
    "    def fast_neg_bp(self, emb_neg_u, emb_neg_v, first_flag):\n",
    "        \"\"\" get grad for negative samples \"\"\"\n",
    "        neg_score = torch.sum(torch.mul(emb_neg_u, emb_neg_v), dim=1)\n",
    "        neg_score = torch.clamp(neg_score, max=6, min=-6)\n",
    "        # [batch_size * negative, 1]\n",
    "        score = - self.fast_sigmoid(neg_score).unsqueeze(1)\n",
    "        if self.record_loss:\n",
    "            if first_flag:\n",
    "                self.loss_fst.append(self.negative * self.neg_weight * torch.mean(self.fast_logsigmoid(-neg_score)).item())\n",
    "            else:\n",
    "                self.loss_snd.append(self.negative * self.neg_weight * torch.mean(self.fast_logsigmoid(-neg_score)).item())\n",
    "\n",
    "        grad_u_neg = self.neg_weight * score * emb_neg_v\n",
    "        grad_v_neg = self.neg_weight * score * emb_neg_u\n",
    "\n",
    "        return grad_u_neg, grad_v_neg\n",
    "\n",
    "    def fast_learn(self, batch_edges, neg_nodes=None):\n",
    "        \"\"\" Learn a batch of edges in a fast way. It has the following features:\n",
    "            1. It calculating the gradients directly without the forward operation.\n",
    "            2. It does sigmoid by a looking up table.\n",
    "        Specifically, for each positive/negative node pair (i,j), the updating procedure is as following:\n",
    "            score = self.fast_sigmoid(u_embedding[i].dot(v_embedding[j]))\n",
    "            # label = 1 for positive samples; label = 0 for negative samples.\n",
    "            u_embedding[i] += (label - score) * v_embedding[j]\n",
    "            v_embedding[i] += (label - score) * u_embedding[j]\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_edges list : a list of node sequnces\n",
    "        neg_nodes torch.LongTensor : a long tensor of sampled true negative nodes. If neg_nodes is None,\n",
    "            then do negative sampling randomly from the nodes in batch_walks as an alternative.\n",
    "        Usage example\n",
    "        -------------\n",
    "        batch_walks = torch.LongTensor([[1,2], [3,4], [5,6]])\n",
    "        neg_nodes = None\n",
    "        \"\"\"\n",
    "        lr = self.lr\n",
    "\n",
    "        # [batch_size, 2]\n",
    "        nodes = batch_edges\n",
    "        if self.only_gpu:\n",
    "            nodes = nodes.to(self.device)\n",
    "            if neg_nodes is not None:\n",
    "                neg_nodes = neg_nodes.to(self.device)\n",
    "        bs = len(nodes)\n",
    "\n",
    "        if self.fst:\n",
    "            emb_u = self.fst_u_embeddings(nodes[:, 0]).view(-1, self.emb_dimension).to(self.device)\n",
    "            emb_v = self.fst_u_embeddings(nodes[:, 1]).view(-1, self.emb_dimension).to(self.device)\n",
    "\n",
    "            ## Postive\n",
    "            emb_pos_u, emb_pos_v = emb_u, emb_v\n",
    "            grad_u_pos, grad_v_pos = self.fast_pos_bp(emb_pos_u, emb_pos_v, True)\n",
    "\n",
    "            ## Negative\n",
    "            emb_neg_u = emb_pos_u.repeat((self.negative, 1))\n",
    "\n",
    "            if bs < self.batch_size:\n",
    "                index_emb_negu, index_emb_negv = init_emb2neg_index(self.negative, bs)\n",
    "                index_emb_negu = index_emb_negu.to(self.device)\n",
    "                index_emb_negv = index_emb_negv.to(self.device)\n",
    "            else:\n",
    "                index_emb_negu = self.index_emb_negu\n",
    "                index_emb_negv = self.index_emb_negv\n",
    "            \n",
    "            if neg_nodes is None:\n",
    "                emb_neg_v = torch.index_select(emb_v, 0, index_emb_negv)\n",
    "            else:\n",
    "                emb_neg_v = self.fst_u_embeddings.weight[neg_nodes].to(self.device)\n",
    "\n",
    "            grad_u_neg, grad_v_neg = self.fast_neg_bp(emb_neg_u, emb_neg_v, True)\n",
    "\n",
    "            ## Update\n",
    "            grad_u_pos.index_add_(0, index_emb_negu, grad_u_neg)\n",
    "            grad_u = grad_u_pos\n",
    "            if neg_nodes is None:\n",
    "                grad_v_pos.index_add_(0, index_emb_negv, grad_v_neg)\n",
    "                grad_v = grad_v_pos\n",
    "            else:\n",
    "                grad_v = grad_v_pos\n",
    "            \n",
    "            # use adam optimizer\n",
    "            grad_u = adam(grad_u, self.fst_state_sum_u, nodes[:, 0], lr, self.device, self.only_gpu)\n",
    "            grad_v = adam(grad_v, self.fst_state_sum_u, nodes[:, 1], lr, self.device, self.only_gpu)\n",
    "            if neg_nodes is not None:\n",
    "                grad_v_neg = adam(grad_v_neg, self.fst_state_sum_u, neg_nodes, lr, self.device, self.only_gpu)\n",
    "\n",
    "            if self.mixed_train:\n",
    "                grad_u = grad_u.cpu()\n",
    "                grad_v = grad_v.cpu()\n",
    "                if neg_nodes is not None:\n",
    "                    grad_v_neg = grad_v_neg.cpu()\n",
    "                else:\n",
    "                    grad_v_neg = None\n",
    "\n",
    "                if self.async_update:\n",
    "                    grad_u.share_memory_()\n",
    "                    grad_v.share_memory_()\n",
    "                    nodes.share_memory_()\n",
    "                    if neg_nodes is not None:\n",
    "                        neg_nodes.share_memory_()\n",
    "                        grad_v_neg.share_memory_()\n",
    "                    self.async_q.put((grad_u, grad_v, grad_v_neg, nodes, neg_nodes, True))\n",
    "            \n",
    "            if not self.async_update:\n",
    "                self.fst_u_embeddings.weight.data.index_add_(0, nodes[:, 0], grad_u)\n",
    "                self.fst_u_embeddings.weight.data.index_add_(0, nodes[:, 1], grad_v)            \n",
    "                if neg_nodes is not None:\n",
    "                    self.fst_u_embeddings.weight.data.index_add_(0, neg_nodes, grad_v_neg)\n",
    "\n",
    "        if self.snd:\n",
    "            emb_u = self.snd_u_embeddings(nodes[:, 0]).view(-1, self.emb_dimension).to(self.device)\n",
    "            emb_v = self.snd_v_embeddings(nodes[:, 1]).view(-1, self.emb_dimension).to(self.device)\n",
    "\n",
    "            ## Postive\n",
    "            emb_pos_u, emb_pos_v = emb_u, emb_v\n",
    "            grad_u_pos, grad_v_pos = self.fast_pos_bp(emb_pos_u, emb_pos_v, False)\n",
    "\n",
    "            ## Negative\n",
    "            emb_neg_u = emb_pos_u.repeat((self.negative, 1))\n",
    "\n",
    "            if bs < self.batch_size:\n",
    "                index_emb_negu, index_emb_negv = init_emb2neg_index(self.negative, bs)\n",
    "                index_emb_negu = index_emb_negu.to(self.device)\n",
    "                index_emb_negv = index_emb_negv.to(self.device)\n",
    "            else:\n",
    "                index_emb_negu = self.index_emb_negu\n",
    "                index_emb_negv = self.index_emb_negv\n",
    "\n",
    "            if neg_nodes is None:\n",
    "                emb_neg_v = torch.index_select(emb_v, 0, index_emb_negv)\n",
    "            else:\n",
    "                emb_neg_v = self.snd_v_embeddings.weight[neg_nodes].to(self.device)\n",
    "\n",
    "            grad_u_neg, grad_v_neg = self.fast_neg_bp(emb_neg_u, emb_neg_v, False)\n",
    "\n",
    "            ## Update\n",
    "            grad_u_pos.index_add_(0, index_emb_negu, grad_u_neg)\n",
    "            grad_u = grad_u_pos\n",
    "            if neg_nodes is None:\n",
    "                grad_v_pos.index_add_(0, index_emb_negv, grad_v_neg)\n",
    "                grad_v = grad_v_pos\n",
    "            else:\n",
    "                grad_v = grad_v_pos\n",
    "            \n",
    "            # use adam optimizer\n",
    "            grad_u = adam(grad_u, self.snd_state_sum_u, nodes[:, 0], lr, self.device, self.only_gpu)\n",
    "            grad_v = adam(grad_v, self.snd_state_sum_v, nodes[:, 1], lr, self.device, self.only_gpu)\n",
    "            if neg_nodes is not None:\n",
    "                grad_v_neg = adam(grad_v_neg, self.snd_state_sum_v, neg_nodes, lr, self.device, self.only_gpu)\n",
    "\n",
    "            if self.mixed_train:\n",
    "                grad_u = grad_u.cpu()\n",
    "                grad_v = grad_v.cpu()\n",
    "                if neg_nodes is not None:\n",
    "                    grad_v_neg = grad_v_neg.cpu()\n",
    "                else:\n",
    "                    grad_v_neg = None\n",
    "\n",
    "                if self.async_update:\n",
    "                    grad_u.share_memory_()\n",
    "                    grad_v.share_memory_()\n",
    "                    nodes.share_memory_()\n",
    "                    if neg_nodes is not None:\n",
    "                        neg_nodes.share_memory_()\n",
    "                        grad_v_neg.share_memory_()\n",
    "                    self.async_q.put((grad_u, grad_v, grad_v_neg, nodes, neg_nodes, False))\n",
    "            \n",
    "            if not self.async_update:\n",
    "                self.snd_u_embeddings.weight.data.index_add_(0, nodes[:, 0], grad_u)\n",
    "                self.snd_v_embeddings.weight.data.index_add_(0, nodes[:, 1], grad_v)            \n",
    "                if neg_nodes is not None:\n",
    "                    self.snd_v_embeddings.weight.data.index_add_(0, neg_nodes, grad_v_neg)\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_embedding(self):\n",
    "        if self.fst:\n",
    "            embedding_fst = self.fst_u_embeddings.weight.cpu().data.numpy()\n",
    "            embedding_fst /= np.sqrt(np.sum(embedding_fst * embedding_fst, 1)).reshape(-1, 1)\n",
    "        if self.snd:\n",
    "            embedding_snd = self.snd_u_embeddings.weight.cpu().data.numpy()\n",
    "            embedding_snd /= np.sqrt(np.sum(embedding_snd * embedding_snd, 1)).reshape(-1, 1)\n",
    "        if self.fst and self.snd:\n",
    "            embedding = np.concatenate((embedding_fst, embedding_snd), 1)\n",
    "            embedding /= np.sqrt(np.sum(embedding * embedding, 1)).reshape(-1, 1)\n",
    "        elif self.fst and not self.snd:\n",
    "            embedding = embedding_fst\n",
    "        elif self.snd and not self.fst:\n",
    "            embedding = embedding_snd\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        return embedding\n",
    "\n",
    "    def save_embedding(self, dataset, file_name):\n",
    "        \"\"\" Write embedding to local file. Only used when node ids are numbers.\n",
    "        Parameter\n",
    "        ---------\n",
    "        dataset DeepwalkDataset : the dataset\n",
    "        file_name str : the file name\n",
    "        \"\"\"\n",
    "        embedding = self.get_embedding()\n",
    "        np.save(file_name, embedding)\n",
    "\n",
    "    def return_embedding(self, dataset):\n",
    "        embedding = torch.Tensor(self.get_embedding()).cpu()\n",
    "        embedding_empty = torch.zeros_like(embedding.data)\n",
    "        valid_nodes = torch.LongTensor(dataset.valid_nodes)\n",
    "        valid_embedding = embedding.data.index_select(0, valid_nodes)\n",
    "        embedding_empty.index_add_(0, valid_nodes, valid_embedding)\n",
    "\n",
    "        return embedding_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6e2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sum_up_params(model):\n",
    "    \"\"\" Count the model parameters \"\"\"\n",
    "    n = []\n",
    "    if model.fst:\n",
    "        p = model.fst_u_embeddings.weight.cpu().data.numel()\n",
    "        n.append(p)\n",
    "        p = model.fst_state_sum_u.cpu().data.numel()\n",
    "        n.append(p)\n",
    "    if model.snd:\n",
    "        p = model.snd_u_embeddings.weight.cpu().data.numel() * 2\n",
    "        n.append(p)\n",
    "        p = model.snd_state_sum_u.cpu().data.numel() * 2\n",
    "        n.append(p)\n",
    "    n.append(model.lookup_table.cpu().numel())\n",
    "    try:\n",
    "        n.append(model.index_emb_negu.cpu().numel() * 2)\n",
    "    except:\n",
    "        pass\n",
    "    print(\"#params \" + str(sum(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97503c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import dgl\n",
    "import dgl.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LineTrainer:\n",
    "    def __init__(self, data, gpus, lr, batch_size, lap_norm, negative, neg_weight, fast_neg, num_samples, num_threads, num_sampler_threads, num_nodes, dim):\n",
    "        \"\"\" Initializing the trainer with the input arguments \"\"\"\n",
    "        self.data = data\n",
    "        self.gpus = [0]\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.lap_norm = lap_norm\n",
    "        self.negative = negative\n",
    "        self.neg_weight = neg_weight\n",
    "        self.fast_neg = fast_neg\n",
    "        self.num_samples = num_samples\n",
    "        self.num_threads = num_threads\n",
    "        self.num_sampler_threads = num_sampler_threads\n",
    "        self.dataset = LineDataset(\n",
    "            data=data,\n",
    "            batch_size=batch_size,\n",
    "            negative=negative,\n",
    "            fast_neg=fast_neg,\n",
    "            num_samples=num_samples * 1000000,\n",
    "            )\n",
    "        self.emb_size = num_nodes\n",
    "        self.dim = dim\n",
    "        self.emb_model = None\n",
    "\n",
    "    def init_device_emb(self):\n",
    "        \"\"\" set the device before training \n",
    "        will be called once in fast_train_mp / fast_train\n",
    "        \"\"\"\n",
    "        # initializing embedding on CPU\n",
    "        self.emb_model = SkipGramModel(\n",
    "            emb_size=self.emb_size, \n",
    "            emb_dimension=self.dim,\n",
    "            batch_size=self.batch_size,\n",
    "            only_cpu=False,\n",
    "            only_gpu=True,\n",
    "            only_fst=False,\n",
    "            only_snd=False,\n",
    "            neg_weight=self.neg_weight,\n",
    "            negative=self.negative,\n",
    "            lr=self.lr,\n",
    "            lap_norm=self.lap_norm,\n",
    "            fast_neg=self.fast_neg,\n",
    "            record_loss=False,\n",
    "            num_threads=self.num_threads,\n",
    "            )\n",
    "        \n",
    "        torch.set_num_threads(self.num_threads)\n",
    "        print(\"Run in 1 GPU\")\n",
    "        assert self.gpus[0] >= 0\n",
    "        self.emb_model.all_to_device(self.gpus[0])\n",
    "            \n",
    "    def train(self):\n",
    "        \"\"\" train the embedding \"\"\"\n",
    "        return self.fast_train()\n",
    "\n",
    "    def fast_train(self):\n",
    "        \"\"\" fast train with dataloader with only gpu / only cpu\"\"\"\n",
    "        self.init_device_emb()\n",
    "\n",
    "        sum_up_params(self.emb_model)\n",
    "\n",
    "        sampler = self.dataset.create_sampler(0)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset=sampler.seeds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=sampler.sample,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=self.num_sampler_threads,\n",
    "            )\n",
    "        \n",
    "        num_batches = len(dataloader)\n",
    "        print(\"num batchs: %d\\n\" % num_batches)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, edges in enumerate(dataloader):\n",
    "                if self.fast_neg:\n",
    "                    self.emb_model.fast_learn(edges)\n",
    "                else:\n",
    "                    # do negative sampling\n",
    "                    bs = edges.size()[0]\n",
    "                    neg_nodes = torch.LongTensor(\n",
    "                        np.random.choice(self.dataset.neg_table, \n",
    "                            bs * self.args.negative, \n",
    "                            replace=True))\n",
    "                    self.emb_model.fast_learn(edges, neg_nodes=neg_nodes)\n",
    "\n",
    "        return self.emb_model.return_embedding(self.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccae8079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE UPDATES: ITERATION 1\n",
      "Finish reading graph\n",
      "generate 3000000000 samples\n",
      "Run in 1 GPU\n",
      "The device is cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W ParallelNative.cpp:212] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m9/khq_m4f11szc3433fj2t17vh0000gn/T/ipykernel_8182/135242757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m                           \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                           dim=100)\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mcl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0miter_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m9/khq_m4f11szc3433fj2t17vh0000gn/T/ipykernel_8182/3823653263.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m\"\"\" train the embedding \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfast_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m9/khq_m4f11szc3433fj2t17vh0000gn/T/ipykernel_8182/3823653263.py\u001b[0m in \u001b[0;36mfast_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfast_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\" fast train with dataloader with only gpu / only cpu\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_device_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0msum_up_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m9/khq_m4f11szc3433fj2t17vh0000gn/T/ipykernel_8182/3823653263.py\u001b[0m in \u001b[0;36minit_device_emb\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run in 1 GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m9/khq_m4f11szc3433fj2t17vh0000gn/T/ipykernel_8182/737144716.py\u001b[0m in \u001b[0;36mall_to_device\u001b[0;34m(self, gpu_id)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;34m\"\"\" move all of the parameters to a single GPU \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfst_u_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfst_u_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m9/khq_m4f11szc3433fj2t17vh0000gn/T/ipykernel_8182/737144716.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(self, gpu_id)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The device is\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsigmoid_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch_geometric import loader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import convert\n",
    "\n",
    "\n",
    "#cluster_data = loader.ClusterData(data, num_parts=1000, recursive=False)\n",
    "#torch.save(cluster_data, \"CLUSTER_DATA.pt\")\n",
    "\n",
    "cluster_data = torch.load(\"CLUSTER_DATA.pt\")\n",
    "\n",
    "# rp = GaussianRandomProjection(n_components=dataset.num_features)\n",
    "# cluster_data.data.x = torch.from_numpy(rp.fit_transform(cluster_data.data.x))\n",
    "iter_num = 1\n",
    "\n",
    "for cl in cluster_data:\n",
    "#         labels = torch.squeeze(cl.y).clone().detach()\n",
    "#         labels[cl.valid_mask] = -1\n",
    "#         labels[cl.test_mask] = -1\n",
    "    print(\"LINE UPDATES: ITERATION \" + str(iter_num))\n",
    "    cl_nx = convert.to_networkx(cl, node_attrs=['x'], to_undirected=True)\n",
    "    cl_dgl = dgl.from_networkx(cl_nx, node_attrs=['x'])\n",
    "    trainer = LineTrainer(data=cl_dgl, \n",
    "                          gpus=[0], \n",
    "                          lr=0.2, \n",
    "                          batch_size=4096, \n",
    "                          lap_norm=0.01, \n",
    "                          negative=1, \n",
    "                          neg_weight=1.0, \n",
    "                          fast_neg=True, \n",
    "                          num_samples=3000, \n",
    "                          num_threads=2, \n",
    "                          num_sampler_threads=2, \n",
    "                          num_nodes=cl.x.shape[0], \n",
    "                          dim=100)\n",
    "    cl.x = trainer.train()\n",
    "    iter_num += 1\n",
    "\n",
    "op_dict = {}\n",
    "op_dict['embedding'] = cluster_data.data.x\n",
    "op_dict['label'] = cluster_data.data.y.to(torch.long)\n",
    "op_dict['train_idx'] = cluster_data.data.train_mask\n",
    "op_dict['valid_idx'] = cluster_data.data.valid_mask\n",
    "op_dict['test_idx'] = cluster_data.data.test_mask\n",
    "\n",
    "torch.save(op_dict, '{}.pt'.format(\"LINE_EMBEDDINGS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6502c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
